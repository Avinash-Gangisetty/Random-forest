{"nbformat": 4, "nbformat_minor": 0, "metadata": {"colab": {"name": "M2W3_024_RandomforestandEnsembleMethods_C.ipynb", "provenance": [], "collapsed_sections": []}, "kernelspec": {"name": "python3", "display_name": "Python 3"}, "accelerator": "GPU"}, "cells": [{"cell_type": "markdown", "metadata": {"id": "iEMEcUqU7r9U", "colab_type": "text"}, "source": ["\n", "# Advanced Certification in AIML\n", "## A Program by IIIT-H and TalentSprint"]}, {"cell_type": "markdown", "metadata": {"id": "oW2tMrARI0fW", "colab_type": "text"}, "source": ["## Learning Objectives"]}, {"cell_type": "markdown", "metadata": {"id": "0I9gpJvkI3eo", "colab_type": "text"}, "source": ["At the end of the experiment, you will be able to:\n", "\n", "                 \n", "\n", "*  Understand the concept of Random Forest \n", "*  Understand Decision Tree classifer and  Ensemble Methods."]}, {"cell_type": "markdown", "metadata": {"id": "Gsq6gtlaJEsc", "colab_type": "text"}, "source": ["## Dataset"]}, {"cell_type": "markdown", "metadata": {"id": "32yp-oYAJGMz", "colab_type": "text"}, "source": ["### Description\n", "\n", "In this experiment we will use a synthetic dataset from sklearn named make_blobs.  The points in the dataset follows Gaussian distribution. It is suitable for linear classification problems.\n", "\n", "\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {"id": "KjKpx8ExJaTw", "colab_type": "text"}, "source": ["## AI /ML  Technique"]}, {"cell_type": "markdown", "metadata": {"id": "G1UfZ3EsJes4", "colab_type": "text"}, "source": ["\n", "In this experiment, while we try to apply the DecisionTreeClassifer(), irrelevant attributes can\tresult in overfitting the training data.\n", "\n", "Note that overfitting occurs when the tree is designed so as to perfectly fit all samples in the training data set. Thus this effects the accuracy when predicting samples that are not part of the training set. \n", "\n", "\n", "To overcome this challenge of overfitting the training data, we apply two Ensemble Methods in this experiment.\n", "\n", "1. Random Forest\n", "2. Bagging\n", "\n", "Rather than just relying on one decision tree and hoping we made a right decision at each split Ensemble Methods allow us to take a sample of Decision Trees into account, calculate which features to use or questions to ask at each split, and make a final predictor based on the aggregated results of the sampled Decision Trees. Ensemble Methods help us to improve algorithm accuracy or improve the robustness of a model. \n", "\n", "### 1. Random Forest\n", "\n", "\n", "A random forest is a collection of decision trees whose results are aggregated into one final result. Random Forest  is a supervised classification algorithm. There is a direct relationship between the number of trees in the forest and the results it can get: the larger the number of trees, the more accurate the result. But here creating the forest is not the same as constructing the decision with information gain or gain index approach.\n", "\n", "The difference between Random Forest algorithm and the decision tree algorithm is that in Random Forest, the process of finding the root node and splitting the feature nodes will run randomly.\n", "\n", "\n", "**Why Random Forest algorithm?**  Overfitting is one critical problem that may make the results worse, but for Random Forest algorithm, if there are enough trees in the forest, the classifier won\u2019t overfit the model. The advantage is the classifier of Random Forest can handle missing values and can be used to model the categorical values.\n", "\n", "\n", "** How does Random Forest algorithm work?** There are two stages in Random Forest algorithm, one is random forest creation, the other is to make a prediction from the random forest classifier created in the first stage. \n", "\n", "** Steps:**\n", "\n", "1. Randomly select \u201ck\u201d features from total \u201cm\u201d features where k << m\n", "2. Among the \u201ck\u201d features, calculate the node \u201cd\u201d using the best split point\n", "3. Split the node into leaf nodes using the best split\n", "4. Repeat the 1 to 3 steps until \u201cl\u201d number of nodes has been reached\n", "5. Build forest by repeating steps 1 to 4 for \u201cn\u201d number times to create \u201cn\u201d number of trees.\n", "6. Take the test features and use the rules of each randomly created decision tree to predict the outcome and stores the predicted outcome (target)\n", "7. Calculate the votes for each predicted target\n", "8. Consider the high voted predicted target as the final prediction from the random forest algorithm\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {"id": "0j12tErYKBsI", "colab_type": "text"}, "source": ["### 2. Bagging"]}, {"cell_type": "markdown", "metadata": {"id": "T2N8icnhJ_6B", "colab_type": "text"}, "source": ["\n", "\n", "Bagging means Bootstrap Aggregation. It is a process of selecting samples from original sample and using these samples for estimating the model accuracy.  \n", "\n", "\n", "Bootstrapping is a process of creating random samples with replacement for estimating sample accuracy.\n", "\n", "One of the way to select samples or bootstrap samples is to select n items with replacement from an original sample, N.  A bootstrap sample may have a few duplicate observations or records, as the sampling is done with replacement.\n", "\n", "In this experiment we apply BaggingClassifier() method from sklearn.  It takes base_estimator, n_estimators, max_samples, max_features, bootstrap_features as parameters. You will be able to understand about the bagging classifier while going through the code of the experiment"]}, {"cell_type": "markdown", "metadata": {"id": "MQt50Bne7r9a", "colab_type": "text"}, "source": ["### Keywords\n", "\n", "* Random forests\n", "* Ensemble methods\n", "* overfitting "]}, {"cell_type": "markdown", "metadata": {"id": "XQdX2Oy27r9b", "colab_type": "text"}, "source": ["### Expected time to complete this experiment is : 60 min"]}, {"cell_type": "code", "metadata": {"id": "gNmFgJUK4LOj", "colab_type": "code", "cellView": "form", "colab": {}}, "source": ["#@title Experiment Explanation Video\n", "from IPython.display import HTML\n", "\n", "HTML(\"\"\"<video width=\"500\" height=\"300\" controls>\n", "  <source src=\"https://cdn.talentsprint.com/aiml/AIML_BATCH_HYD_7/10march/module_2_week_8_experment_1.mp4\" type=\"video/mp4\">\n", "</video>\n", "\"\"\")"], "execution_count": 0, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "L5C60MtW72-G", "colab_type": "text"}, "source": ["### Setup Steps"]}, {"cell_type": "code", "metadata": {"id": "DJEbP21u75Go", "colab_type": "code", "colab": {}}, "source": ["#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n", "Id = \"\" #@param {type:\"string\"}\n"], "execution_count": 0, "outputs": []}, {"cell_type": "code", "metadata": {"id": "7cDKDrjT778i", "colab_type": "code", "colab": {}}, "source": ["#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n", "password = \"\" #@param {type:\"string\"}\n"], "execution_count": 0, "outputs": []}, {"cell_type": "code", "metadata": {"id": "ZjyDySHZKcw7", "colab_type": "code", "cellView": "form", "colab": {}}, "source": ["#@title Run this cell to complete the setup for this Notebook\n", "from IPython import get_ipython\n", "\n", "ipython = get_ipython()\n", "  \n", "notebook= \"M2W3_024_RandomforestandEnsembleMethods_C\" #name of the notebook\n", "\n", "def setup():\n", "#  ipython.magic(\"sx pip3 install torch\")  \n", "    from IPython.display import HTML, display\n", "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n", "    print(\"Setup completed successfully\")\n", "    return\n", "\n", "\n", "def submit_notebook():\n", "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n", "    \n", "    import requests, json, base64, datetime\n", "\n", "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n", "    if not submission_id:\n", "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n", "      r = requests.post(url, data = data)\n", "      r = json.loads(r.text)\n", "\n", "      if r[\"status\"] == \"Success\":\n", "          return r[\"record_id\"]\n", "      elif \"err\" in r:        \n", "        print(r[\"err\"])\n", "        return None        \n", "      else:\n", "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n", "        return None\n", "    \n", "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getWalkthrough() and getComments() and getInclassSupport() and getOnlineSupport():\n", "      f = open(notebook + \".ipynb\", \"rb\")\n", "      file_hash = base64.b64encode(f.read())\n", "\n", "      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n", "              \"concepts\" : Concepts, \"record_id\" : submission_id, \n", "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n", "              \"notebook\" : notebook, \"feedback_walkthrough\":Walkthrough ,\n", "              \"feedback_experiments_input\" : Comments,\n", "              \"feedback_inclass_mentor\": Inclass_support,\n", "              \"feedback_online_mentor\" : Online_support}\n", "\n", "      r = requests.post(url, data = data)\n", "      r = json.loads(r.text)\n", "      if \"err\" in r:        \n", "        print(r[\"err\"])\n", "        return None   \n", "      else:\n", "        print(\"Your submission is successful.\")\n", "        print(\"Ref Id:\", submission_id)\n", "        print(\"Date of submission: \", r[\"date\"])\n", "        print(\"Time of submission: \", r[\"time\"])\n", "        print(\"View your submissions: https://iiith-aiml.talentsprint.com/notebook_submissions\")\n", "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n", "        return submission_id\n", "    else: submission_id\n", "    \n", "\n", "def getAdditional():\n", "  try:\n", "    if not Additional: \n", "      raise NameError\n", "    else:\n", "      return Additional  \n", "  except NameError:\n", "    print (\"Please answer Additional Question\")\n", "    return None\n", "\n", "def getComplexity():\n", "  try:\n", "    if not Complexity:\n", "      raise NameError\n", "    else:\n", "      return Complexity\n", "  except NameError:\n", "    print (\"Please answer Complexity Question\")\n", "    return None\n", "  \n", "def getConcepts():\n", "  try:\n", "    if not Concepts:\n", "      raise NameError\n", "    else:\n", "      return Concepts\n", "  except NameError:\n", "    print (\"Please answer Concepts Question\")\n", "    return None\n", "  \n", "  \n", "def getWalkthrough():\n", "  try:\n", "    if not Walkthrough:\n", "      raise NameError\n", "    else:\n", "      return Walkthrough\n", "  except NameError:\n", "    print (\"Please answer Walkthrough Question\")\n", "    return None\n", "  \n", "def getComments():\n", "  try:\n", "    if not Comments:\n", "      raise NameError\n", "    else:\n", "      return Comments\n", "  except NameError:\n", "    print (\"Please answer Comments Question\")\n", "    return None\n", "  \n", "def getInclassSupport():\n", "  try:\n", "    if not Inclass_support:\n", "      raise NameError\n", "    else:\n", "      return Inclass_support\n", "  except NameError:\n", "    print (\"Please answer Inclass support Question\")\n", "    return None\n", "  \n", "  \n", "def getOnlineSupport():\n", "  try:\n", "    if not Online_support:\n", "      raise NameError\n", "    else:\n", "      return Online_support\n", "  except NameError:\n", "    print (\"Please answer Online support Question\")\n", "    return None\n", "\n", "def getAnswer():\n", "  try:\n", "    if not Answer:\n", "      raise NameError \n", "    else: \n", "      return Answer\n", "  except NameError:\n", "    print (\"Please answer Question\")\n", "    return None\n", "  \n", "\n", "def getId():\n", "  try: \n", "    return Id if Id else None\n", "  except NameError:\n", "    return None\n", "\n", "def getPassword():\n", "  try:\n", "    return password if password else None\n", "  except NameError:\n", "    return None\n", "\n", "submission_id = None\n", "### Setup \n", "if getPassword() and getId():\n", "  submission_id = submit_notebook()\n", "  if submission_id:\n", "    setup() \n", "else:\n", "  print (\"Please complete Id and Password cells before running setup\")\n"], "execution_count": 0, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "7BPUIpBu7r9c", "colab_type": "text"}, "source": ["#### Importing the Required Packages"]}, {"cell_type": "code", "metadata": {"id": "3XfRm7PB7r9d", "colab_type": "code", "colab": {}}, "source": ["%matplotlib inline\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns; sns.set()"], "execution_count": 0, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "L6lOFxUU7r9j", "colab_type": "text"}, "source": ["#### Creating a decision tree"]}, {"cell_type": "code", "metadata": {"id": "Puxkh5Xw7r9j", "colab_type": "code", "colab": {}}, "source": ["#Consider the following two-dimensional data, which has one of four class labels:\n", "\n", "from sklearn.datasets import make_blobs\n", "\n", "X, y = make_blobs(n_samples=300, centers=4,\n", "                  random_state=0, cluster_std=1.0)\n", "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow');\n", "\n"], "execution_count": 0, "outputs": []}, {"cell_type": "code", "metadata": {"id": "h3u2-Baw7r9t", "colab_type": "code", "colab": {}}, "source": ["from sklearn.tree import DecisionTreeClassifier\n", "tree = DecisionTreeClassifier().fit(X, y)"], "execution_count": 0, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "kAWKfrmd7r9w", "colab_type": "text"}, "source": ["\n", "\n", "Let's write a quick utility function to help us visualize the output of the classifier:\n"]}, {"cell_type": "code", "metadata": {"id": "93ojAYYq7r9x", "colab_type": "code", "colab": {}}, "source": ["def visualize_classifier(model, X, y, ax=None, cmap='rainbow'):\n", "    ax = ax or plt.gca()\n", "    \n", "    # Plot the training points\n", "    ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cmap,\n", "               clim=(y.min(), y.max()), zorder=3)\n", "    ax.axis('tight')\n", "    ax.axis('off')\n", "    xlim = ax.get_xlim()\n", "    ylim = ax.get_ylim()\n", "    \n", "    # fit the estimator\n", "    model.fit(X, y)\n", "    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n", "                         np.linspace(*ylim, num=200))\n", "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n", "\n", "    # Create a color plot with the results\n", "    n_classes = len(np.unique(y))\n", "    contours = ax.contourf(xx, yy, Z, alpha=0.3,\n", "                           levels=np.arange(n_classes + 1) - 0.5,\n", "                           cmap=cmap, clim=(y.min(), y.max()),\n", "                           zorder=1)\n", "\n", "    ax.set(xlim=xlim, ylim=ylim)\n"], "execution_count": 0, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "b2DYkkfa7r91", "colab_type": "text"}, "source": ["\n", "\n", "#### Now we can examine what the decision tree classification looks like:\n"]}, {"cell_type": "markdown", "metadata": {"id": "jI7DrgZ37r9r", "colab_type": "text"}, "source": ["\n", "\n", "A simple decision tree built on this data will iteratively split the data along one or the other axis according to some quantitative criterion, and at each level assign the label of the new region according to a majority vote of points within it. \n", "\n", "Notice that after the first split, every point in the upper branch remains unchanged, so there is no need to further subdivide this branch. Except for nodes that contain all of one color, at each level every region is again split along one of the two features.\n", "\n", "This process of fitting a decision tree to our data can be done in Scikit-Learn with the DecisionTreeClassifier estimator:\n"]}, {"cell_type": "code", "metadata": {"id": "YFn6g9pP7r93", "colab_type": "code", "colab": {}}, "source": ["visualize_classifier(DecisionTreeClassifier(), X, y)"], "execution_count": 0, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "KY6aTYJx7r98", "colab_type": "text"}, "source": ["Such over-fitting turns out to be a general property of decision trees: it is very easy to go too deep in the tree, and thus to fit details of the particular data rather than the overall properties of the distributions they are drawn from. Another way to see this over-fitting is to look at models trained on different subsets of the data\u2014for example, in this figure we train two different trees, each on half of the original data:\n", "\n", "\n", "\n", "![alt text](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.08-decision-tree-overfitting.png)"]}, {"cell_type": "markdown", "metadata": {"id": "hy-789Wa7r9-", "colab_type": "text"}, "source": ["It is clear that in some places, the two trees produce consistent results (e.g., in the four corners), while in other places, the two trees give very different classifications (e.g., in the regions between any two clusters). The key observation is that the inconsistencies tend to happen where the classification is less certain, and thus by using information from both of these trees, we might come up with a better result!"]}, {"cell_type": "markdown", "metadata": {"id": "eT6_vNUk7r9_", "colab_type": "text"}, "source": ["\n", "### Ensembles of Estimators: Random Forests\n", "\n", "This notion\u2014that multiple overfitting estimators can be combined to reduce the effect of this overfitting\u2014is what underlies an ensemble method called bagging. Bagging makes use of an ensemble (a grab bag, perhaps) of parallel estimators, each of which over-fits the data, and averages the results to find a better classification. An ensemble of randomized decision trees is known as a random forest.\n", "\n", "This type of bagging classification can be done manually using Scikit-Learn's BaggingClassifier meta-estimator, as shown here:\n"]}, {"cell_type": "code", "metadata": {"id": "JwsYPLE77r-B", "colab_type": "code", "colab": {}}, "source": ["from sklearn.tree import DecisionTreeClassifier\n", "from sklearn.ensemble import BaggingClassifier\n", "\n", "tree = DecisionTreeClassifier()\n", "bag = BaggingClassifier(tree, n_estimators=100, max_samples=0.8,\n", "                        random_state=1)\n", "\n", "bag.fit(X, y)\n", "visualize_classifier(bag, X, y)"], "execution_count": 0, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "zZpSArhG7r-G", "colab_type": "text"}, "source": ["\n", "\n", "In this example, we have randomized the data by fitting each estimator with a random subset of 80% of the training points. In practice, decision trees are more effectively randomized by injecting some stochasticity in how the splits are chosen: this way all the data contributes to the fit each time, but the results of the fit still have the desired randomness. For example, when determining which feature to split on, the randomized tree might select from among the top several features. You can read more technical details about these randomization strategies in the Scikit-Learn documentation and references within.\n", "\n", "In Scikit-Learn, such an optimized ensemble of randomized decision trees is implemented in the RandomForestClassifier estimator, which takes care of all the randomization automatically. All you need to do is select a number of estimators, and it will very quickly (in parallel, if desired) fit the ensemble of trees:\n"]}, {"cell_type": "code", "metadata": {"id": "YoI_E7XK7r-H", "colab_type": "code", "colab": {}}, "source": ["from sklearn.ensemble import RandomForestClassifier\n", "\n", "model = RandomForestClassifier(n_estimators=100, random_state=0)\n", "visualize_classifier(model, X, y);"], "execution_count": 0, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "9N2X7Pw77r-N", "colab_type": "text"}, "source": ["We see that by averaging over 100 randomly perturbed models, we end up with an overall model that is much closer to our intuition about how the parameter space should be split."]}, {"cell_type": "markdown", "metadata": {"id": "YNXOFRNiLgtS", "colab_type": "text"}, "source": ["**Ungraded Exercise :**\n", " \n", "* Classify the data in this experiment independently with (Use sklearn)\n", "    * Linear classifier\n", "    * SVM \n", "    * MLP\n", "    * Decision tree\n", "\n", "* Store the predictions  for each sample from every classifier\n", "* Create a new classifier, which classifies sample based on the above trained classifiers\n", "  * Classification Rule : Consider that the sample belongs to the majority class out of the prediction from all the above mentioned classifiers, In case of tie Chose randomly\n", "  * print the accuracies of Linear classifier, SVM, MLP, Decision tree and the Majority voting classifier, you created."]}, {"cell_type": "code", "metadata": {"id": "YwEmdWOsOXaD", "colab_type": "code", "colab": {}}, "source": ["### YOUR CODE HERE"], "execution_count": 0, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "YDmrqqzR8ZIT", "colab_type": "text"}, "source": ["### Please answer the questions below to complete the experiment:"]}, {"cell_type": "code", "metadata": {"id": "Y5mma0_4zydO", "colab_type": "code", "colab": {}}, "source": ["#@title In Random Forest, the idea of aggregation (as a part of bagging) relates to the voting of several Decision trees?{ run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n", "Answer = \"\" #@param [\"\",\"TRUE\",\"FALSE\"]\n"], "execution_count": 0, "outputs": []}, {"cell_type": "code", "metadata": {"id": "14AC4vu-_0_e", "colab_type": "code", "colab": {}}, "source": ["#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n", "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"], "execution_count": 0, "outputs": []}, {"cell_type": "code", "metadata": {"id": "jA_EwOrVBdqC", "colab_type": "code", "colab": {}}, "source": ["#@title If it was very easy, what more you would have liked to have been added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n", "Additional = \"\" #@param {type:\"string\"}"], "execution_count": 0, "outputs": []}, {"cell_type": "code", "metadata": {"id": "4VBk_4VTAxCM", "colab_type": "code", "colab": {}}, "source": ["#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n", "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]"], "execution_count": 0, "outputs": []}, {"cell_type": "code", "metadata": {"id": "r35isHfTVGKc", "colab_type": "code", "colab": {}}, "source": ["#@title  Experiment walkthrough video? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n", "Walkthrough = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"], "execution_count": 0, "outputs": []}, {"cell_type": "code", "metadata": {"id": "XH91cL1JWH7m", "colab_type": "code", "colab": {}}, "source": ["#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n", "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"], "execution_count": 0, "outputs": []}, {"cell_type": "code", "metadata": {"id": "z8xLqj7VWIKW", "colab_type": "code", "colab": {}}, "source": ["#@title In class Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n", "Inclass_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"], "execution_count": 0, "outputs": []}, {"cell_type": "code", "metadata": {"id": "fVEvrDcTWIZY", "colab_type": "code", "colab": {}}, "source": ["#@title Online Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n", "Online_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"], "execution_count": 0, "outputs": []}, {"cell_type": "code", "metadata": {"id": "FzAZHt1zw-Y-", "colab_type": "code", "cellView": "form", "colab": {}}, "source": ["#@title Run this cell to submit your notebook for grading { vertical-output: true }\n", "try:\n", "  if submission_id:\n", "      return_id = submit_notebook()\n", "      if return_id : submission_id =return_id\n", "  else:\n", "      print(\"Please complete the setup first.\")\n", "except NameError:\n", "  print (\"Please complete the setup first.\")"], "execution_count": 0, "outputs": []}]}